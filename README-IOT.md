# Pipeline IoT para PrevisÃ£o de Chuvas ğŸŒ¦ï¸

Sistema completo de pipeline IoT para coleta, processamento e previsÃ£o de dados meteorolÃ³gicos usando Apache Airflow, PostgreSQL, MongoDB, MinIO e Machine Learning.

## ğŸ—ï¸ Arquitetura

```mermaid
graph TB
    A[OpenWeatherMap API] --> B[Weather Fetch]
    B --> C[MongoDB]
    C --> D[Weather Hour Aggregator]
    D --> E[PostgreSQL]
    E --> F[Weather Processor]
    F --> G[City Predictor ML]
    G --> H[PrediÃ§Ãµes]
    
    I[Apache Airflow] --> B
    I --> D
    I --> F
    I --> G
    
    J[MinIO] --> F
    K[Redis] --> I
```

## ğŸš€ Funcionalidades

- **Coleta de Dados**: ExtraÃ§Ã£o automÃ¡tica de dados meteorolÃ³gicos da API OpenWeatherMap
- **Armazenamento**: 
  - MongoDB para dados brutos (JSON)
  - PostgreSQL para dados estruturados
  - MinIO para armazenamento de objetos
- **Processamento**: AgregaÃ§Ã£o horÃ¡ria e limpeza de dados usando Apache Spark
- **Machine Learning**: Modelos SARIMAX para previsÃ£o de temperatura por cidade
- **OrquestraÃ§Ã£o**: Pipeline automatizado com Apache Airflow
- **Monitoramento**: Logging estruturado e mÃ©tricas de performance

## ğŸ“‹ PrÃ©-requisitos

- Docker e Docker Compose
- Python 3.10+
- Make (opcional, mas recomendado)

## ğŸ› ï¸ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1. Clone o repositÃ³rio
```bash
git clone <repository-url>
cd IOT
```

### 2. Configure as variÃ¡veis de ambiente
```bash
cp .env.example .env
# Edite o arquivo .env com suas configuraÃ§Ãµes
```

### 3. Inicie os serviÃ§os
```bash
# Usando Make (recomendado)
make start

# Ou usando Docker Compose diretamente
docker-compose -f docker-compose-iot.yml up -d
```

### 4. Configure o ambiente Python (desenvolvimento local)
```bash
make setup-env
source venv/bin/activate
```

## ğŸ”§ Uso

### Pipeline Completo (Local)
```bash
# Execute o pipeline completo
make run-pipeline

# Ou diretamente
python main.py
```

### Componentes Individuais
```bash
# Apenas coleta de dados
make run-fetch

# Apenas agregaÃ§Ã£o horÃ¡ria
make run-aggregate

# Executar testes
make test
```

### Interface Web

- **Airflow UI**: http://localhost:8080 (admin/admin123)
- **MinIO Console**: http://localhost:9001 (minioadmin/minioadmin123)

## ğŸ“Š Fluxo de Dados

### 1. ExtraÃ§Ã£o (Weather Fetch)
- Coleta dados de 50+ cidades globalmente
- Salva dados brutos no MongoDB
- FrequÃªncia: A cada hora

### 2. AgregaÃ§Ã£o (Weather Hour)
- Processa dados do MongoDB usando Spark
- Agrega por coordenadas e hora
- Salva dados estruturados no PostgreSQL

### 3. Processamento (Weather Processor)
- Limpeza e validaÃ§Ã£o de dados
- Feature engineering
- PreparaÃ§Ã£o para ML

### 4. Machine Learning (City Predictor)
- Modelos SARIMAX por cidade
- PrevisÃ£o de temperatura 24h
- MÃ©tricas de performance (MAE, RMSE, RÂ²)

## ğŸ—ï¸ Estrutura do Projeto

```
IOT/
â”œâ”€â”€ src/                          # CÃ³digo fonte
â”‚   â”œâ”€â”€ weather_fetch.py         # Coleta de dados da API
â”‚   â”œâ”€â”€ weather_hour.py          # AgregaÃ§Ã£o horÃ¡ria com Spark
â”‚   â”œâ”€â”€ config.py                # ConfiguraÃ§Ãµes centralizadas
â”‚   â”œâ”€â”€ data_processing/         # Processamento de dados
â”‚   â”‚   â””â”€â”€ weather_processor.py
â”‚   â”œâ”€â”€ ml/                      # Modelos de Machine Learning
â”‚   â”‚   â””â”€â”€ city_predictor.py
â”‚   â””â”€â”€ utils/                   # UtilitÃ¡rios
â”‚       â”œâ”€â”€ logger.py
â”‚       â”œâ”€â”€ database.py
â”‚       â””â”€â”€ minio_client.py
â”œâ”€â”€ dags/                        # DAGs do Airflow
â”‚   â””â”€â”€ weather_iot_pipeline_dag.py
â”œâ”€â”€ docker/                      # ConfiguraÃ§Ãµes Docker
â”‚   â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ init-db.sql
â”œâ”€â”€ notebooks/                   # Jupyter Notebooks
â”œâ”€â”€ tests/                       # Testes unitÃ¡rios
â”œâ”€â”€ main.py                      # Script principal
â”œâ”€â”€ docker-compose-iot.yml       # ConfiguraÃ§Ã£o Docker
â”œâ”€â”€ Makefile                     # Comandos automatizados
â””â”€â”€ requirements-pipeline.txt    # DependÃªncias Python
```

## ğŸ” Monitoramento e Logs

### Visualizar Logs
```bash
# Todos os serviÃ§os
make logs

# ServiÃ§o especÃ­fico
make logs-postgres
make logs-mongodb
make logs-minio
```

### Status dos ServiÃ§os
```bash
make status
```

### Monitoramento de Recursos
```bash
make monitor
```

## ğŸ“ˆ MÃ©tricas de ML

O sistema registra automaticamente:
- **MAE** (Mean Absolute Error): Erro mÃ©dio absoluto
- **RMSE** (Root Mean Square Error): Raiz do erro quadrÃ¡tico mÃ©dio
- **RÂ²** (Coeficiente de DeterminaÃ§Ã£o): Qualidade do ajuste

## ğŸ§ª Testes

```bash
# Todos os testes
make test

# Apenas testes unitÃ¡rios
make test-unit

# Com cobertura de cÃ³digo
pytest tests/ --cov=src --cov-report=html
```

## ğŸ³ Docker Services

| ServiÃ§o | Porta | DescriÃ§Ã£o |
|---------|-------|-----------|
| PostgreSQL | 5432 | Banco principal para dados estruturados |
| MongoDB | 27017 | Armazenamento de dados brutos (JSON) |
| MinIO | 9000/9001 | Armazenamento de objetos |
| Redis | 6379 | Cache e message broker |
| Airflow DB | 5433 | Banco do Airflow |

## ğŸ”§ Desenvolvimento

### ConfiguraÃ§Ã£o de Desenvolvimento
```bash
make dev-setup
```

### Reset Completo
```bash
make dev-reset
```

### FormataÃ§Ã£o de CÃ³digo
```bash
make format
make lint
```

## ğŸ“ ConfiguraÃ§Ã£o AvanÃ§ada

### Spark Configuration
- Executor Memory: 2GB (configurÃ¡vel via .env)
- Driver Memory: 1GB (configurÃ¡vel via .env)
- Master: local[*] (usa todos os cores disponÃ­veis)

### Airflow Configuration
- Schedule: @hourly (a cada hora)
- Max Active Runs: 1
- Retries: 3 com delay de 5 minutos

## ğŸš¨ Troubleshooting

### Problemas Comuns

1. **Erro de permissÃµes nos logs**:
   ```bash
   chmod 755 logs/
   # ou use o diretÃ³rio alternativo automaticamente
   ```

2. **Erro de conexÃ£o com banco**:
   ```bash
   make start  # Certifique-se que os serviÃ§os estÃ£o rodando
   make status # Verifique status dos containers
   ```

3. **Erro no Spark**:
   ```bash
   # Verifique se o Java estÃ¡ instalado corretamente
   docker-compose -f docker-compose-iot.yml logs mongodb
   ```

### Backup e RestauraÃ§Ã£o
```bash
# Backup do banco
make backup-db

# Acesso direto aos bancos
make shell-postgres
make shell-mongodb
```

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a MIT License.

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie sua feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“ Suporte

Para suporte e dÃºvidas, consulte:
- DocumentaÃ§Ã£o do Airflow: https://airflow.apache.org/docs/
- DocumentaÃ§Ã£o do Spark: https://spark.apache.org/docs/latest/
- Issues do GitHub: [Link para issues]
