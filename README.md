# Pipeline IoT para PrevisÃ£o de Chuvas

Um pipeline completo para coletar, processar e analisar dados climÃ¡ticos em tempo real e prever se vai chover.

## ğŸŒ¦ï¸ VisÃ£o Geral

Este projeto implementa um pipeline de dados IoT completo para previsÃ£o de chuvas, incluindo:
- **Coleta de dados** de fontes climÃ¡ticas (Kaggle)
- **Processamento ETL** com Apache Airflow
- **Armazenamento** em PostgreSQL e MinIO
- **Machine Learning** com Scikit-learn

## ğŸš€ Stack TecnolÃ³gica

- **Dados**: Kaggle Weather Dataset
- **Processamento**: Python, Pandas, Apache Airflow
- **Armazenamento**: PostgreSQL (relacional) + MinIO (Data Lake)
- **Machine Learning**: Scikit-learn (classificaÃ§Ã£o binÃ¡ria)
- **OrquestraÃ§Ã£o**: Docker Compose

## ğŸ“ Estrutura do Projeto

```
IOT/
â”œâ”€â”€ dags/                   # Airflow DAGs
â”œâ”€â”€ data/                   # Dados brutos e processados
â”œâ”€â”€ models/                 # Modelos de ML
â”œâ”€â”€ src/                    # CÃ³digo fonte
â”‚   â”œâ”€â”€ data_processing/    # ETL scripts
â”‚   â”œâ”€â”€ ml/                 # Machine Learning
â”‚   â””â”€â”€ utils/              # UtilitÃ¡rios
â”œâ”€â”€ docker/                 # ConfiguraÃ§Ãµes Docker
â”œâ”€â”€ notebooks/              # Jupyter notebooks
â””â”€â”€ tests/                  # Testes
```

## ğŸ”§ ConfiguraÃ§Ã£o do Ambiente

### PrÃ©-requisitos
- Python 3.10+ (testado com Python 3.12)
- Docker e Docker Compose
- Git

### 1. Instalar dependÃªncias do sistema (Ubuntu/Debian)
```bash
# Instalar pip e venv
sudo apt update
sudo apt install python3-pip python3-venv python3-dev

# Instalar Docker (se nÃ£o estiver instalado)
sudo apt install docker.io docker-compose

# Adicionar usuÃ¡rio ao grupo docker
sudo usermod -aG docker $USER
# Fazer logout e login novamente
```

### 2. Criar ambiente virtual
```bash
cd /home/nicholas/projects/IOT
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
# venv\Scripts\activate   # Windows
```

### 3. Instalar dependÃªncias Python
```bash
pip install -r requirements.txt
```

### 4. Configurar variÃ¡veis de ambiente
```bash
cp .env.example .env
# Editar .env com suas configuraÃ§Ãµes
nano .env
```

### 5. Subir serviÃ§os com Docker
```bash
# OpÃ§Ã£o 1: Usar Docker Compose
docker-compose up -d

# OpÃ§Ã£o 2: Usar Makefile
make docker-up
make setup  # Para setup completo incluindo Airflow
```

### 6. Executar pipeline (desenvolvimento local)
```bash
# Executar pipeline principal
python main.py

# Ou usar Makefile
make run
```

## ğŸŒ ServiÃ§os

- **Airflow**: http://localhost:8080
- **MinIO**: http://localhost:9001
- **PostgreSQL**: localhost:5432

## ğŸ“Š Funcionalidades

### Pipeline ETL
- Coleta automÃ¡tica de dados climÃ¡ticos
- Limpeza e normalizaÃ§Ã£o dos dados
- Armazenamento em Data Warehouse

### Machine Learning
- Modelo de classificaÃ§Ã£o binÃ¡ria (chuva/nÃ£o chuva)
- Treinamento automatizado
- AvaliaÃ§Ã£o de performance


## ğŸ” Uso

1. **Dados**: Coloque os dados do Kaggle na pasta `data/raw/`
2. **Treinamento**: Execute o pipeline para treinar o modelo
3. **PrevisÃ£o**: Use o dashboard para fazer previsÃµes
4. **Monitoramento**: Acompanhe no Airflow

## ğŸ¯ Como Usar

### Desenvolvimento Local
```bash
# 1. Gerar dados de exemplo
python generate_sample_data.py

# 2. Executar pipeline
python main.py
```

### ProduÃ§Ã£o com Docker
```bash
# 1. Subir todos os serviÃ§os
make setup

# 2. Acessar interfaces
# - Airflow: http://localhost:8080 (admin/admin)
# - MinIO: http://localhost:9001 (minioadmin/minioadmin123)
```

### Comandos Ãšteis
```bash
make help          # Ver todos os comandos
make test          # Executar testes
make docker-logs   # Ver logs dos serviÃ§os
make clean         # Limpar arquivos temporÃ¡rios
```

## ğŸ“Š Estrutura de Dados

### Dados de Entrada
- **Temperatura**: Â°C
- **Umidade**: %
- **PressÃ£o**: hPa
- **Vento**: velocidade (km/h) e direÃ§Ã£o
- **PrecipitaÃ§Ã£o**: mm
- **LocalizaÃ§Ã£o**: cidades brasileiras

### Features Derivadas
- **SensaÃ§Ã£o tÃ©rmica**
- **Ponto de orvalho**
- **Ãndice de calor**
- **TendÃªncia de pressÃ£o**
- **Features temporais** (hora, dia da semana, mÃªs, estaÃ§Ã£o)

### Target
- **will_rain**: classificaÃ§Ã£o binÃ¡ria (0/1)

## ğŸ” Monitoramento

### MÃ©tricas do Modelo
- **AUC-ROC**: Ãrea sob a curva ROC
- **PrecisÃ£o**: Verdadeiros positivos / (VP + Falsos positivos)
- **Recall**: Verdadeiros positivos / (VP + Falsos negativos)
- **F1-Score**: MÃ©dia harmÃ´nica entre precisÃ£o e recall

### Pipeline de Dados
- **Qualidade**: VerificaÃ§Ã£o de valores nulos e outliers
- **LatÃªncia**: Tempo de processamento
- **Volume**: Quantidade de dados processados
- **Frescar**: Timestamp dos Ãºltimos dados

## ğŸ“ˆ PrÃ³ximos Passos

- [ ] IntegraÃ§Ã£o com APIs de clima em tempo real (OpenWeatherMap, INMET)
- [ ] Modelos mais avanÃ§ados (XGBoost, Deep Learning)
- [ ] Alertas automÃ¡ticos via email/SMS
- [ ] API REST para previsÃµes
- [ ] Deploy em nuvem (AWS, GCP, Azure)
- [ ] Monitoramento com Prometheus/Grafana
- [ ] CI/CD com GitHub Actions
- [ ] DocumentaÃ§Ã£o automÃ¡tica com Sphinx

## ğŸ§ª Testes

```bash
# Executar todos os testes
python -m pytest tests/ -v

# Com coverage
python -m pytest tests/ -v --cov=src --cov-report=html

# Testes especÃ­ficos
python -m pytest tests/test_weather_processor.py -v
```

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### Guidelines
- Siga o padrÃ£o PEP 8 para Python
- Adicione testes para novas funcionalidades
- Documente novas funÃ§Ãµes e classes
- Use type hints sempre que possÃ­vel

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## ğŸ™ Agradecimentos

- [Apache Airflow](https://airflow.apache.org/) - OrquestraÃ§Ã£o de workflows
- [Scikit-learn](https://scikit-learn.org/) - Machine Learning
- [PostgreSQL](https://www.postgresql.org/) - Banco de dados
- [MinIO](https://min.io/) - Object Storage
- [Docker](https://www.docker.com/) - ContainerizaÃ§Ã£o

---

Desenvolvido com â¤ï¸ para a comunidade de Data Science e IoT
